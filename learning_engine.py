"""
–£–õ–¨–¢–†–ê-–û–ë–£–ß–ê–Æ–©–ê–Ø –°–ò–°–¢–ï–ú–ê –î–õ–Ø MLBB –ë–û–¢–ê
–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç Deep Reinforcement Learning, –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ –∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ
"""

import json
import time
import threading
import pickle
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field, asdict
from collections import defaultdict, deque
import random
from pathlib import Path
import torch
import torch.nn as nn
import torch.optim as optim

@dataclass
class NeuralNetworkModel:
    """–ü—Ä–æ—Å—Ç–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–æ—Å—Ç–æ—è–Ω–∏–π"""
    def __init__(self, input_size: int, hidden_size: int, output_size: int):
        self.net = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Linear(hidden_size // 2, output_size)
        )
        self.optimizer = optim.Adam(self.net.parameters(), lr=0.001)
        self.loss_fn = nn.MSELoss()
    
    def predict(self, state: np.ndarray) -> np.ndarray:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ Q-–∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è —Å–æ—Å—Ç–æ—è–Ω–∏–π"""
        with torch.no_grad():
            tensor_state = torch.FloatTensor(state)
            return self.net(tensor_state).numpy()
    
    def train(self, states: np.ndarray, targets: np.ndarray, epochs: int = 5):
        """–û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏"""
        states_tensor = torch.FloatTensor(states)
        targets_tensor = torch.FloatTensor(targets)
        
        for epoch in range(epochs):
            self.optimizer.zero_grad()
            predictions = self.net(states_tensor)
            loss = self.loss_fn(predictions, targets_tensor)
            loss.backward()
            self.optimizer.step()
        
        return loss.item()

@dataclass
class UltraLearningData:
    """–°–≤–µ—Ä—Ö-–¥–∞–Ω–Ω—ã–µ –¥–ª—è —É–ª—å—Ç—Ä–∞-–æ–±—É—á–µ–Ω–∏—è"""
    experiences: List[Dict] = field(default_factory=list)
    trajectories: List[List[Dict]] = field(default_factory=list)
    q_table: Dict[str, Dict[str, float]] = field(default_factory=dict)
    success_patterns: Dict[str, Dict] = field(default_factory=dict)
    failure_patterns: Dict[str, Dict] = field(default_factory=dict)
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
    learning_metrics: Dict[str, List[float]] = field(default_factory=lambda: {
        'rewards': [],
        'success_rate': [],
        'exploration_rate': [],
        'loss': []
    })
    
    def add_experience(self, experience: Dict):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ–ø—ã—Ç–∞"""
        self.experiences.append(experience)
        if len(self.experiences) > 100000:  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏
            self.experiences = self.experiences[-50000:]
    
    def add_trajectory(self, trajectory: List[Dict]):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ (–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–µ–π—Å—Ç–≤–∏–π)"""
        self.trajectories.append(trajectory)
        if len(self.trajectories) > 1000:
            self.trajectories = self.trajectories[-500:]
    
    def update_q_value(self, state: str, action: str, value: float, alpha: float = 0.1):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ Q-–∑–Ω–∞—á–µ–Ω–∏—è —Å —É—á–µ—Ç–æ–º —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è"""
        if state not in self.q_table:
            self.q_table[state] = {}
        
        old_value = self.q_table[state].get(action, 0.0)
        self.q_table[state][action] = old_value + alpha * (value - old_value)
    
    def get_best_action(self, state: str) -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ª—É—á—à–µ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
        if state not in self.q_table or not self.q_table[state]:
            return None
        
        return max(self.q_table[state].items(), key=lambda x: x[1])[0]
    
    def get_action_value(self, state: str, action: str) -> float:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏—è –¥–ª—è —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
        return self.q_table.get(state, {}).get(action, 0.0)

class UltraLearningEngine:
    """–£–ª—å—Ç—Ä–∞-–ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –¥–≤–∏–∂–æ–∫ –æ–±—É—á–µ–Ω–∏—è —Å —Ä–µ–∏–Ω—Ñ–æ—Ä—Å–º–µ–Ω—Ç –ª–µ—Ä–Ω–∏–Ω–≥–æ–º"""
    
    def __init__(self, data_dir: str = "ultra_data", use_neural: bool = True):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
        self.data = UltraLearningData()
        self.use_neural = use_neural
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã RL
        self.gamma = 0.95  # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        self.alpha = 0.2   # –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
        self.epsilon = 0.3  # –ù–∞—á–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
        self.epsilon_decay = 0.995
        self.epsilon_min = 0.05
        self.batch_size = 32
        
        # –ù–µ–π—Ä–æ—Å–µ—Ç—å –¥–ª—è Deep Q-Learning
        if use_neural and torch.cuda.is_available():
            print("üéÆ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CUDA –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è")
            self.device = torch.device("cuda")
        elif use_neural:
            print("üíª –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è")
            self.device = torch.device("cpu")
        else:
            print("üìä –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–∞–±–ª–∏—á–Ω—ã–π Q-Learning")
            self.device = None
        
        if use_neural:
            self.dqn = NeuralNetworkModel(input_size=15, hidden_size=128, output_size=9)
            self.target_net = NeuralNetworkModel(input_size=15, hidden_size=128, output_size=9)
            self.update_target_net()
            self.replay_buffer = deque(maxlen=10000)
        
        # –ú–∞–ø–ø–∏–Ω–≥ –¥–µ–π—Å—Ç–≤–∏–π –∫ –∏–Ω–¥–µ–∫—Å–∞–º
        self.action_map = {
            'farm': 0, 'gank': 1, 'jungle': 2, 'retreat': 3,
            'patrol': 4, 'teamfight': 5, 'objective': 6, 
            'defend': 7, 'push': 8
        }
        self.reverse_action_map = {v: k for k, v in self.action_map.items()}
        
        # –¢—Ä–µ–∫–µ—Ä—ã –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        self.recent_rewards = deque(maxlen=100)
        self.success_history = deque(maxlen=50)
        self.exploration_history = []
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        self.load_ultra_data()
        
        # –ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        self.auto_save_thread(300)
        
        print(f"üöÄ –£–õ–¨–¢–†–ê-–°–ò–°–¢–ï–ú–ê –û–ë–£–ß–ï–ù–ò–Ø –ê–ö–¢–ò–í–ò–†–û–í–ê–ù–ê")
        print(f"üß† –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {'–Ω–µ–π—Ä–æ—Å–µ—Ç—å' if use_neural else 'Q-—Ç–∞–±–ª–∏—Ü–∞'}")
        print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.data.experiences)} –æ–ø—ã—Ç–æ–≤ –∏ {len(self.data.trajectories)} —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π")
    
    def state_to_vector(self, state: Dict) -> np.ndarray:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –≤ –≤–µ–∫—Ç–æ—Ä –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏"""
        vector = np.zeros(15, dtype=np.float32)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        vector[0] = state.get('health', 100) / 100.0
        vector[1] = state.get('level', 1) / 15.0
        vector[2] = state.get('gold', 300) / 10000.0
        vector[3] = min(state.get('enemies_nearby', 0) / 5.0, 1.0)
        vector[4] = min(state.get('creeps_nearby', 0) / 10.0, 1.0)
        vector[5] = min(state.get('jungle_creeps_nearby', 0) / 5.0, 1.0)
        vector[6] = state.get('safety_score', 1.0)
        
        # One-hot –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–∑—ã
        phases = ['early', 'mid', 'late', 'endgame']
        phase = state.get('phase', 'early')
        phase_idx = phases.index(phase) if phase in phases else 0
        vector[7 + phase_idx] = 1.0
        
        # One-hot –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–∏
        positions = ['base', 'ally_territory', 'jungle', 'enemy_territory']
        position = state.get('position', 'ally_territory')
        pos_idx = positions.index(position) if position in positions else 0
        vector[11 + pos_idx] = 1.0
        
        return vector
    
    def calculate_reward(self, state: Dict, action: str, result: Dict, next_state: Dict) -> float:
        """–†–∞—Å—á–µ—Ç –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è —Å —É—á–µ—Ç–æ–º –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —Ñ–∞–∫—Ç–æ—Ä–æ–≤"""
        reward = 0.0
        
        # –ë–∞–∑–æ–≤–æ–µ –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –∑–∞ —É—Å–ø–µ—Ö
        if result.get('success', False):
            reward += 5.0
        
        # –í–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –∑–∞ —É—Ä–æ–Ω
        damage_reward = result.get('damage_dealt', 0) / 100.0
        reward += damage_reward
        
        # –í–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –∑–∞ –∑–æ–ª–æ—Ç–æ
        gold_reward = result.get('gold_earned', 0) / 50.0
        reward += gold_reward
        
        # –í–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –∑–∞ —É–±–∏–π—Å—Ç–≤–∞
        kill_reward = result.get('kills', 0) * 10.0
        reward += kill_reward
        
        # –®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–ª—É—á–µ–Ω–Ω—ã–π —É—Ä–æ–Ω
        damage_taken_penalty = result.get('damage_taken', 0) / 50.0
        reward -= damage_taken_penalty
        
        # –í–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –∑–∞ –≤—ã–∂–∏–≤–∞–Ω–∏–µ
        if next_state.get('health', 100) > 30:
            reward += 1.0
        
        # –í–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –∑–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏
        time_penalty = result.get('time_taken', 0) / 10.0
        reward -= time_penalty
        
        # –í–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –∑–∞ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ —Ü–µ–ª–µ–π
        if result.get('objective_completed', False):
            reward += 20.0
        
        # –í–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –∑–∞ –∫–æ–º–∞–Ω–¥–Ω—É—é –∏–≥—Ä—É
        if result.get('team_assist', False):
            reward += 3.0
        
        return reward
    
    def record_ultra_experience(self, state: Dict, action: str, result: Dict, 
                               next_state: Dict, context: Dict = None):
        """–ó–∞–ø–∏—Å—å —É–ª—å—Ç—Ä–∞-–æ–ø—ã—Ç–∞ —Å —Ä–µ–∏–Ω—Ñ–æ—Ä—Å–º–µ–Ω—Ç –ª–µ—Ä–Ω–∏–Ω–≥–æ–º"""
        try:
            if context is None:
                context = {}
            
            # –†–∞—Å—á–µ—Ç –≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è
            reward = self.calculate_reward(state, action, result, next_state)
            self.recent_rewards.append(reward)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—ã—Ç–∞ –¥–ª—è RL
            experience = {
                'state': state,
                'action': action,
                'reward': reward,
                'next_state': next_state,
                'done': next_state.get('health', 100) <= 0,
                'timestamp': time.time(),
                'context': context
            }
            
            self.data.add_experience(experience)
            
            # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —ç—Ç–æ–º –æ–ø—ã—Ç–µ
            self.learn_from_experience(experience)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —É—Å–ø–µ—Ö–∞/–Ω–µ—É–¥–∞—á–∏
            self.update_success_patterns(state, action, result, reward)
            
            # –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ epsilon (–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ/–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ)
            self.adapt_exploration_rate(reward)
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Å–ø–µ—à–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π
            if reward > 10.0:
                print(f"üèÜ –£–õ–¨–¢–†–ê-–£–°–ü–ï–•: {action.upper()} –Ω–∞–≥—Ä–∞–¥–∞: {reward:.1f}")
            
            # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ –≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ
            if len(self.data.experiences) % 100 == 0:
                self.deep_train()
            
            return reward
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ —É–ª—å—Ç—Ä–∞-–æ–ø—ã—Ç–∞: {e}")
            return 0.0
    
    def learn_from_experience(self, experience: Dict):
        """–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–¥–Ω–æ–º –æ–ø—ã—Ç–µ —Å Q-learning"""
        state = experience['state']
        action = experience['action']
        reward = experience['reward']
        next_state = experience['next_state']
        done = experience['done']
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–ª—é—á–∏ —Å–æ—Å—Ç–æ—è–Ω–∏–π
        state_key = self._create_state_key(state)
        next_state_key = self._create_state_key(next_state)
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–µ–µ Q-–∑–Ω–∞—á–µ–Ω–∏–µ
        current_q = self.data.get_action_value(state_key, action)
        
        if done:
            # –ï—Å–ª–∏ —ç–ø–∏–∑–æ–¥ –∑–∞–∫–æ–Ω—á–µ–Ω, Q-–∑–Ω–∞—á–µ–Ω–∏–µ —Ä–∞–≤–Ω–æ –Ω–∞–≥—Ä–∞–¥–µ
            target_q = reward
        else:
            # –ò–Ω–∞—á–µ —É—á–∏—Ç—ã–≤–∞–µ–º –±—É–¥—É—â–∏–µ –Ω–∞–≥—Ä–∞–¥—ã
            next_best_q = 0.0
            best_next_action = self.data.get_best_action(next_state_key)
            if best_next_action:
                next_best_q = self.data.get_action_value(next_state_key, best_next_action)
            
            target_q = reward + self.gamma * next_best_q
        
        # –û–±–Ω–æ–≤–ª—è–µ–º Q-—Ç–∞–±–ª–∏—Ü—É
        self.data.update_q_value(state_key, action, target_q, self.alpha)
        
        # –î–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–æ–±–∞–≤–ª—è–µ–º –≤ –±—É—Ñ–µ—Ä –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è
        if self.use_neural:
            state_vector = self.state_to_vector(state)
            next_state_vector = self.state_to_vector(next_state)
            action_idx = self.action_map.get(action, 0)
            
            replay_experience = (
                state_vector,
                action_idx,
                reward,
                next_state_vector,
                done
            )
            self.replay_buffer.append(replay_experience)
    
    def update_success_patterns(self, state: Dict, action: str, result: Dict, reward: float):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —É—Å–ø–µ—Ö–∞ –∏ –Ω–µ—É–¥–∞—á–∏"""
        state_key = self._create_state_key(state)
        pattern_key = f"{state_key}_{action}"
        
        if reward > 5.0:  # –£—Å–ø–µ—à–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
            if pattern_key not in self.data.success_patterns:
                self.data.success_patterns[pattern_key] = {
                    'count': 0,
                    'total_reward': 0.0,
                    'avg_reward': 0.0,
                    'last_success': time.time()
                }
            
            pattern = self.data.success_patterns[pattern_key]
            pattern['count'] += 1
            pattern['total_reward'] += reward
            pattern['avg_reward'] = pattern['total_reward'] / pattern['count']
            pattern['last_success'] = time.time()
            
        elif reward < -5.0:  # –ù–µ—É–¥–∞—á–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
            if pattern_key not in self.data.failure_patterns:
                self.data.failure_patterns[pattern_key] = {
                    'count': 0,
                    'total_reward': 0.0,
                    'avg_reward': 0.0,
                    'last_failure': time.time()
                }
            
            pattern = self.data.failure_patterns[pattern_key]
            pattern['count'] += 1
            pattern['total_reward'] += reward
            pattern['avg_reward'] = pattern['total_reward'] / pattern['count']
            pattern['last_failure'] = time.time()
    
    def adapt_exploration_rate(self, reward: float):
        """–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è"""
        self.success_history.append(reward > 0)
        
        # –ï—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ—Å—Ç—å —Ä–∞—Å—Ç–µ—Ç, —É–º–µ–Ω—å—à–∞–µ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ
        if len(self.success_history) > 20:
            success_rate = sum(self.success_history) / len(self.success_history)
            if success_rate > 0.7:
                self.epsilon *= 0.99
            elif success_rate < 0.3:
                self.epsilon *= 1.01
        
        # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –≥—Ä–∞–Ω–∏—Ü—ã epsilon
        self.epsilon = max(self.epsilon_min, min(self.epsilon, 1.0))
        
        # –ó–∞–ø–∏—Å—å –∏—Å—Ç–æ—Ä–∏–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
        self.exploration_history.append(self.epsilon)
        if len(self.exploration_history) > 1000:
            self.exploration_history = self.exploration_history[-1000:]
    
    def select_ultra_action(self, state: Dict, possible_actions: List[str]) -> Tuple[str, float]:
        """–í—ã–±–æ—Ä —É–ª—å—Ç—Ä–∞-–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è —Å –±–∞–ª–∞–Ω—Å–æ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è/–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
        try:
            # Epsilon-greedy —Å—Ç—Ä–∞—Ç–µ–≥–∏—è
            if random.random() < self.epsilon:
                # –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ: —Å–ª—É—á–∞–π–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ
                action = random.choice(possible_actions)
                confidence = self.epsilon
                return action, confidence
            
            # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: –≤—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ
            state_key = self._create_state_key(state)
            
            if self.use_neural and len(self.replay_buffer) >= self.batch_size:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–µ–π—Ä–æ—Å–µ—Ç—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏
                state_vector = self.state_to_vector(state)
                with torch.no_grad():
                    q_values = self.dqn.predict(state_vector.reshape(1, -1))[0]
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –≤–æ–∑–º–æ–∂–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è
                action_scores = {}
                for action in possible_actions:
                    action_idx = self.action_map.get(action)
                    if action_idx is not None:
                        action_scores[action] = q_values[action_idx]
                
                if action_scores:
                    best_action = max(action_scores.items(), key=lambda x: x[1])[0]
                    best_score = action_scores[best_action]
                    confidence = min(1.0, best_score / 10.0)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                    return best_action, confidence
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Q-—Ç–∞–±–ª–∏—Ü—É
            best_action = None
            best_q = -float('inf')
            
            for action in possible_actions:
                q_value = self.data.get_action_value(state_key, action)
                if q_value > best_q:
                    best_q = q_value
                    best_action = action
            
            if best_action is None:
                best_action = random.choice(possible_actions)
                confidence = 0.3
            else:
                confidence = min(1.0, (best_q + 10) / 20.0)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            
            return best_action, confidence
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤—ã–±–æ—Ä–∞ —É–ª—å—Ç—Ä–∞-–¥–µ–π—Å—Ç–≤–∏—è: {e}")
            return random.choice(possible_actions), 0.3
    
    def deep_train(self):
        """–ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –Ω–∞ –±—É—Ñ–µ—Ä–µ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è"""
        if not self.use_neural or len(self.replay_buffer) < self.batch_size:
            return
        
        try:
            # –í—ã–±–æ—Ä–∫–∞ –∏–∑ –±—É—Ñ–µ—Ä–∞ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è
            batch = random.sample(self.replay_buffer, self.batch_size)
            
            states = np.zeros((self.batch_size, 15))
            actions = np.zeros(self.batch_size, dtype=np.int64)
            rewards = np.zeros(self.batch_size)
            next_states = np.zeros((self.batch_size, 15))
            dones = np.zeros(self.batch_size, dtype=np.bool_)
            
            for i, (state, action, reward, next_state, done) in enumerate(batch):
                states[i] = state
                actions[i] = action
                rewards[i] = reward
                next_states[i] = next_state
                dones[i] = done
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Ü–µ–ª–µ–≤—ã–µ Q-–∑–Ω–∞—á–µ–Ω–∏—è
            with torch.no_grad():
                next_q_values = self.target_net.predict(next_states)
                max_next_q = np.max(next_q_values, axis=1)
            
            target_q = rewards + self.gamma * max_next_q * (~dones)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ç–µ–∫—É—â–µ–π —Å–µ—Ç–∏
            current_q_values = self.dqn.predict(states)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ Q-–∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π
            for i in range(self.batch_size):
                current_q_values[i, actions[i]] = target_q[i]
            
            # –û–±—É—á–µ–Ω–∏–µ —Å–µ—Ç–∏
            loss = self.dqn.train(states, current_q_values, epochs=3)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
            self.data.learning_metrics['loss'].append(loss)
            
            # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π —Å–µ—Ç–∏
            if len(self.data.experiences) % 1000 == 0:
                self.update_target_net()
                print(f"üîÑ –¶–µ–ª–µ–≤–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–∞, loss: {loss:.4f}")
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è: {e}")
    
    def update_target_net(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ (soft update)"""
        if not self.use_neural:
            return
        
        # Soft update: –æ–±–Ω–æ–≤–ª—è–µ–º —Ü–µ–ª–µ–≤—É—é —Å–µ—Ç—å –º–µ–¥–ª–µ–Ω–Ω–æ
        target_params = self.target_net.net.state_dict()
        source_params = self.dqn.net.state_dict()
        
        tau = 0.001  # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –º—è–≥–∫–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
        for key in source_params:
            target_params[key] = target_params[key] * (1 - tau) + source_params[key] * tau
        
        self.target_net.net.load_state_dict(target_params)
    
    def record_trajectory(self, trajectory: List[Dict]):
        """–ó–∞–ø–∏—Å—å –ø–æ–ª–Ω–æ–π —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ (–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–æ—Å—Ç–æ—è–Ω–∏–π-–¥–µ–π—Å—Ç–≤–∏–π)"""
        self.data.add_trajectory(trajectory)
        
        # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏
        self.learn_from_trajectory(trajectory)
    
    def learn_from_trajectory(self, trajectory: List[Dict]):
        """–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –ø–æ–ª–Ω–æ–π —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏ (Monte Carlo)"""
        if len(trajectory) < 2:
            return
        
        # –í—ã—á–∏—Å–ª—è–µ–º –≤–æ–∑–≤—Ä–∞—Ç—ã (returns) —Å –∫–æ–Ω—Ü–∞ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏
        returns = 0
        for i in range(len(trajectory) - 1, -1, -1):
            experience = trajectory[i]
            reward = experience.get('reward', 0)
            returns = reward + self.gamma * returns
            
            # –û–±–Ω–æ–≤–ª—è–µ–º Q-–∑–Ω–∞—á–µ–Ω–∏–µ —Å —É—á–µ—Ç–æ–º –æ–±—â–µ–≥–æ –≤–æ–∑–≤—Ä–∞—Ç–∞
            state = experience.get('state', {})
            action = experience.get('action', '')
            state_key = self._create_state_key(state)
            self.data.update_q_value(state_key, action, returns, self.alpha * 0.5)
    
    def get_ultra_recommendations(self, state: Dict, top_n: int = 3) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —É–ª—å—Ç—Ä–∞-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Å –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ–º"""
        recommendations = []
        state_key = self._create_state_key(state)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —É—Å–ø–µ—à–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ—Ö–æ–∂–∏—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π
        similar_patterns = self.find_similar_patterns(state_key)
        
        for pattern_key, pattern_data in similar_patterns[:top_n]:
            parts = pattern_key.split('_')
            if len(parts) >= 2:
                action = parts[-1]  # –ü–æ—Å–ª–µ–¥–Ω—è—è —á–∞—Å—Ç—å - –¥–µ–π—Å—Ç–≤–∏–µ
                state_part = '_'.join(parts[:-1])
                
                recommendations.append({
                    'action': action,
                    'confidence': pattern_data.get('avg_reward', 0) / 10.0,
                    'success_rate': pattern_data.get('count', 0) / max(pattern_data.get('count', 1), 1),
                    'reason': f"–£—Å–ø–µ—à–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω: {pattern_data.get('count', 0)} —É—Å–ø–µ—Ö–æ–≤",
                    'avg_reward': pattern_data.get('avg_reward', 0)
                })
        
        return sorted(recommendations, key=lambda x: x['confidence'], reverse=True)
    
    def find_similar_patterns(self, state_key: str, threshold: float = 0.7) -> List[Tuple[str, Dict]]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        similar = []
        state_parts = state_key.split('_')
        
        for pattern_key, pattern_data in self.data.success_patterns.items():
            pattern_parts = pattern_key.split('_')
            
            # –ü—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏
            common_parts = len(set(state_parts) & set(pattern_parts[:-1]))
            similarity = common_parts / max(len(set(state_parts)), len(set(pattern_parts[:-1])))
            
            if similarity >= threshold:
                similar.append((pattern_key, pattern_data))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
        similar.sort(key=lambda x: x[1].get('avg_reward', 0), reverse=True)
        return similar
    
    def save_ultra_data(self, filename: str = None):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —É–ª—å—Ç—Ä–∞-–¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è"""
        if filename is None:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            filename = self.data_dir / f"ultra_learning_{timestamp}.pkl"
        
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
            data_to_save = {
                'data': self.data,
                'epsilon': self.epsilon,
                'recent_rewards': list(self.recent_rewards),
                'exploration_history': self.exploration_history,
                'timestamp': time.time()
            }
            
            with open(filename, 'wb') as f:
                pickle.dump(data_to_save, f)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
            if self.use_neural:
                torch.save(self.dqn.net.state_dict(), self.data_dir / "dqn_model.pth")
                torch.save(self.target_net.net.state_dict(), self.data_dir / "target_model.pth")
            
            print(f"üíæ –£–ª—å—Ç—Ä–∞-–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —É–ª—å—Ç—Ä–∞-–¥–∞–Ω–Ω—ã—Ö: {e}")
    
    def load_ultra_data(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —É–ª—å—Ç—Ä–∞-–¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è"""
        try:
            # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ñ–∞–π–ª
            ultra_files = list(self.data_dir.glob("ultra_learning_*.pkl"))
            if not ultra_files:
                print("üìÇ –£–ª—å—Ç—Ä–∞-–¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –Ω–∞—á–∏–Ω–∞–µ–º —Å –Ω—É–ª—è")
                return
            
            latest_file = max(ultra_files, key=lambda x: x.stat().st_mtime)
            
            with open(latest_file, 'rb') as f:
                loaded_data = pickle.load(f)
            
            self.data = loaded_data.get('data', UltraLearningData())
            self.epsilon = loaded_data.get('epsilon', 0.3)
            self.recent_rewards = deque(loaded_data.get('recent_rewards', []), maxlen=100)
            self.exploration_history = loaded_data.get('exploration_history', [])
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
            if self.use_neural:
                dqn_path = self.data_dir / "dqn_model.pth"
                target_path = self.data_dir / "target_model.pth"
                
                if dqn_path.exists():
                    self.dqn.net.load_state_dict(torch.load(dqn_path))
                    print("üß† DQN –Ω–µ–π—Ä–æ—Å–µ—Ç—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                
                if target_path.exists():
                    self.target_net.net.load_state_dict(torch.load(target_path))
                    print("üéØ –¶–µ–ª–µ–≤–∞—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
            print(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω—ã —É–ª—å—Ç—Ä–∞-–¥–∞–Ω–Ω—ã–µ –∏–∑ {latest_file.name}")
            print(f"üìä –û–ø—ã—Ç–æ–≤: {len(self.data.experiences)}, "
                  f"–¢—Ä–∞–µ–∫—Ç–æ—Ä–∏–π: {len(self.data.trajectories)}, "
                  f"–£—Å–ø–µ—à–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {len(self.data.success_patterns)}")
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —É–ª—å—Ç—Ä–∞-–¥–∞–Ω–Ω—ã—Ö: {e}")
            print("üîÑ –ù–∞—á–∏–Ω–∞–µ–º —Å –Ω—É–ª—è")
            self.data = UltraLearningData()
    
    def auto_save_thread(self, interval: int = 300):
        """–ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö"""
        def save_loop():
            while True:
                time.sleep(interval)
                self.save_ultra_data()
                self.print_progress()
        
        thread = threading.Thread(target=save_loop, daemon=True)
        thread.start()
        print(f"‚è±Ô∏è –ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —É–ª—å—Ç—Ä–∞-–¥–∞–Ω–Ω—ã—Ö –∫–∞–∂–¥—ã–µ {interval} —Å–µ–∫—É–Ω–¥")
    
    def print_progress(self):
        """–í—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
        if not self.recent_rewards:
            return
        
        avg_reward = np.mean(list(self.recent_rewards))
        success_rate = len([r for r in self.recent_rewards if r > 0]) / len(self.recent_rewards)
        
        print(f"\nüìà –ü–†–û–ì–†–ï–°–° –£–õ–¨–¢–†–ê-–û–ë–£–ß–ï–ù–ò–Ø:")
        print(f"   –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {avg_reward:.2f}")
        print(f"   –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {success_rate:.1%}")
        print(f"   –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ (epsilon): {self.epsilon:.3f}")
        print(f"   –û–ø—ã—Ç–æ–≤: {len(self.data.experiences)}")
        print(f"   –£—Å–ø–µ—à–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {len(self.data.success_patterns)}")
        print(f"   Q-–∑–∞–ø–∏—Å–µ–π: {sum(len(v) for v in self.data.q_table.values())}")
        
        if self.data.learning_metrics.get('loss'):
            avg_loss = np.mean(self.data.learning_metrics['loss'][-10:])
            print(f"   –ü–æ—Ç–µ—Ä—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏: {avg_loss:.4f}")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        if self.data.success_patterns:
            top_patterns = sorted(
                self.data.success_patterns.items(),
                key=lambda x: x[1].get('avg_reward', 0),
                reverse=True
            )[:3]
            
            print(f"\nüèÜ –¢–û–ü-3 –ø–∞—Ç—Ç–µ—Ä–Ω–∞:")
            for i, (pattern, data) in enumerate(top_patterns, 1):
                print(f"   {i}. {pattern}: –Ω–∞–≥—Ä–∞–¥–∞={data.get('avg_reward', 0):.1f}, "
                      f"–ø–æ–ø—ã—Ç–æ–∫={data.get('count', 0)}")
    
    def _create_state_key(self, state: Dict) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–ª—é—á–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
        key_parts = [
            f"h{int(state.get('health', 0))}",
            f"l{state.get('level', 1)}",
            f"e{state.get('enemies_nearby', 0)}",
            f"c{state.get('creeps_nearby', 0)}",
            f"j{state.get('jungle_creeps_nearby', 0)}",
            f"s{int(state.get('safety_score', 1.0) * 10)}",
            f"g{int(state.get('gold', 0) / 100)}",
            f"p{state.get('phase', 'early')[:1]}",
            f"pos{state.get('position', 'unknown')[:3]}"
        ]
        return "_".join(key_parts)
    
    def get_learning_insights(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Å–∞–π—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è"""
        if not self.data.experiences:
            return {}
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –æ–ø—ã—Ç–æ–≤
        recent_experiences = self.data.experiences[-100:]
        recent_rewards = [exp.get('reward', 0) for exp in recent_experiences]
        
        # –ê–Ω–∞–ª–∏–∑ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –ø–æ –¥–µ–π—Å—Ç–≤–∏—è–º
        action_success = defaultdict(list)
        for exp in recent_experiences:
            action = exp.get('action', '')
            reward = exp.get('reward', 0)
            action_success[action].append(reward > 0)
        
        action_stats = {}
        for action, successes in action_success.items():
            if successes:
                action_stats[action] = {
                    'success_rate': sum(successes) / len(successes),
                    'count': len(successes),
                    'avg_reward': np.mean([exp.get('reward', 0) 
                                         for exp in recent_experiences 
                                         if exp.get('action') == action])
                }
        
        # –¢—Ä–µ–Ω–¥—ã
        if len(self.data.learning_metrics.get('success_rate', [])) > 10:
            recent_success = self.data.learning_metrics['success_rate'][-10:]
            success_trend = np.polyfit(range(len(recent_success)), recent_success, 1)[0]
        else:
            success_trend = 0
        
        return {
            'avg_reward': np.mean(recent_rewards) if recent_rewards else 0,
            'reward_std': np.std(recent_rewards) if recent_rewards else 0,
            'action_stats': dict(action_stats),
            'exploration_rate': self.epsilon,
            'success_trend': success_trend,
            'total_experiences': len(self.data.experiences),
            'unique_patterns': len(self.data.success_patterns),
            'learning_progress': min(100, len(self.data.experiences) / 1000 * 100)  # –ü—Ä–æ—Ü–µ–Ω—Ç –æ–±—É—á–µ–Ω–∏—è
        }

# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Å–∏—Å—Ç–µ–º–æ–π
def integrate_ultra_learning(bot_core_instance):
    """–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —É–ª—å—Ç—Ä–∞-–æ–±—É—á–µ–Ω–∏—è —Å –æ—Å–Ω–æ–≤–Ω—ã–º –±–æ—Ç–æ–º"""
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É–ª—å—Ç—Ä–∞-–¥–≤–∏–∂–∫–∞
    ultra_engine = UltraLearningEngine(data_dir="ultra_learning_data", use_neural=True)
    
    # –ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è –º–µ—Ç–æ–¥–∞ game_cycle
    original_game_cycle = bot_core_instance.game_cycle
    
    def ultra_game_cycle():
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        initial_state = bot_core_instance.state.__dict__.copy()
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –æ–±—ã—á–Ω—ã–π —Ü–∏–∫–ª
        result = original_game_cycle()
        
        # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        new_state = bot_core_instance.state.__dict__.copy()
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —É–ª—å—Ç—Ä–∞-–æ–ø—ã—Ç
        if hasattr(bot_core_instance, 'last_action'):
            reward = ultra_engine.record_ultra_experience(
                state=initial_state,
                action=bot_core_instance.last_action,
                result=result,
                next_state=new_state,
                context={
                    'phase': bot_core_instance.state.phase,
                    'position': bot_core_instance.state.map_position
                }
            )
            
            # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –≤—ã–±–æ—Ä –¥–µ–π—Å—Ç–≤–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–∏—è
            if random.random() < 0.3:  # 30% chance to use ultra learning
                possible_actions = ['farm', 'gank', 'jungle', 'retreat', 'patrol']
                ultra_action, confidence = ultra_engine.select_ultra_action(
                    state=initial_state,
                    possible_actions=possible_actions
                )
                
                if confidence > 0.6:
                    bot_core_instance.last_action = ultra_action
                    print(f"üéØ –£–õ–¨–¢–†–ê-–í–´–ë–û–†: {ultra_action} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.1%})")
        
        return result
    
    # –ó–∞–º–µ–Ω—è–µ–º –º–µ—Ç–æ–¥
    bot_core_instance.game_cycle = ultra_game_cycle
    
    # –î–æ–±–∞–≤–ª—è–µ–º —É–ª—å—Ç—Ä–∞-–¥–≤–∏–∂–æ–∫ –≤ —ç–∫–∑–µ–º–ø–ª—è—Ä
    bot_core_instance.ultra_engine = ultra_engine
    
    print("üöÄ –£–õ–¨–¢–†–ê-–û–ë–£–ß–ï–ù–ò–ï –ò–ù–¢–ï–ì–†–ò–†–û–í–ê–ù–û –í –ë–û–¢–ê")
    return bot_core_instance